{
  "name": "llama3-ko-8b-gguf-q4km",
  "backend": "gguf",
  "model": {
    "repo_id": "QuantFactory/Llama-3-Ko-8B-Instruct-GGUF",
    "filename": "Llama-3-Ko-8B-Instruct.Q4_K_M.gguf",
    "cache_dir_env": "HF_CACHE_DIR"
  },
  "load_params": {
    "n_ctx": 4096,
    "n_threads": 8,
    "n_gpu_layers": 0,
    "chat_format": "llama-3"
  },
  "generation": {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 40,
    "repetition_penalty": 1.1,
    "stop": []
  },
  "prompts": {
    "roles": [
      {
        "role": "system",
        "content": "당신은 대한민국의 대학교 인프라 예측 서비스 Libra의 도우미 '리브라봇'입니다. 모든 답변은 최대 2줄, 깔끔하고 친절하게. 모든 문장 끝에 '냥!'을 붙입니다."
      },
      {
        "role": "assistant",
        "content": "안녕하세요, 리브라봇이에요. 무엇을 도와드릴까냥!"
      }
    ],
    "variables": {}
  },
  "policy": {
    "enforce_max_lines": 2,
    "force_suffix": "냥!"
  }
}

{
  "name": "llama3-ko-8b-gguf-q4km",
  "backend": "gguf",
  "model": {
    "repo_id": "QuantFactory/Llama-3-Ko-8B-Instruct-GGUF",
    "filename": "Llama-3-Ko-8B-Instruct.Q4_K_M.gguf",
    "cache_dir_env": "HF_CACHE_DIR"
  },
  "load_params": {
    "n_ctx": 2000,
    "n_threads": 8,
    "n_gpu_layers": 0,
    "chat_format": "llama-3"
  },
  "generation": {
    "max_new_tokens": 128,
    "temperature": 0.9,
    "top_p": 0.9,
    "top_k": 40,
    "repetition_penalty": 1.1,
    "stop": []
  },
  "policy": {
    "enforce_max_lines": 0,
    "force_suffix": ""
  },
  "multiturn": {
    "ctx_strategy": "budgeted_compact",
    "token_per_char": 0.6,
    "reserve_tokens": 256
  }
}
